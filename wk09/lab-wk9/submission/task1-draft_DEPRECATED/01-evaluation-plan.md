# Task 1: Evaluation Plan

**Student ID**: [Your student ID]
**Date**: [Date]
**Module**: COMP2850 HCI

---

## 1. Evaluation Objectives

**What are you evaluating?**
- [Brief description of the system/interface being evaluated]
- [Key features under evaluation]

**Why evaluate now?**
- [Design questions you need answered]
- [Decisions that depend on evaluation results]

---

## 2. Link to Needs-Finding (LO2)

**Connection to Week 6 job stories**:
- [Reference 2-3 job stories from Week 6 Lab 2 that informed your evaluation tasks]
- [Example: "Job story: 'When I need to track coursework deadlines, I want to quickly add tasks, so I don't forget important dates' → informed Task 3 (add task) design"]

**How needs-finding shaped evaluation**:
- [Explain how user needs identified in Week 6 influenced which features to prioritise for testing]
- [Example: "Needs-finding revealed that participants value speed over visual polish, so we're measuring time-on-task as primary metric"]

**Evidence trail**: Week 6 job stories → backlog items → features implemented → evaluation tasks

---

## 3. Evaluation Method Selection

**Method chosen**: Task-based usability testing

**Rationale**:
- [Why task-based testing is appropriate for your design questions]
- [What alternative methods were considered and why rejected]

**Target participants**: Peer students (n=4 minimum)

---

## 4. Success Criteria

**Quantitative metrics**:
- **Time-on-task**: Target < [X] seconds for Task 1, < [Y] seconds for Task 2, etc.
- **Completion rate**: Target ≥ 80% (participants complete without critical errors)
- **Error rate**: Target < 20% (validation errors, incorrect submissions)
- **Confidence**: Target ≥ 4/5 average (post-task self-rating)

**Qualitative indicators**:
- Participants can complete tasks without asking for help
- No confusion about interface affordances (buttons, links, form fields)
- Errors are recoverable without frustration
- No accessibility barriers (keyboard traps, missing announcements, invisible focus)

---

## 5. Evaluation Scope

**In scope**:
- [Feature 1: e.g., Task creation with validation]
- [Feature 2: e.g., Filtering/search functionality]
- [Feature 3: e.g., Inline editing]
- [Feature 4: e.g., Delete with confirmation]

**Out of scope** (not evaluated in this round):
- [Features deferred to later evaluation]
- [Known limitations accepted as constraints]

---

## 6. Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Insufficient participants (n<4) | Can't identify patterns | Recruit early, offer flexible scheduling |
| Technical failures during pilots | Lost data, incomplete sessions | Test setup before pilots, have backup device |
| Pilot bias (peers know the system) | Overly positive results | Use structured tasks, observe actions not opinions |
| Privacy breach (PII in logs) | GDPR violation | Use anonymous session IDs, no names/emails in data |

---

## 7. Ethical Considerations

**Consent**: ✅ Participants will read and confirm consent protocol before starting
**Right to withdraw**: ✅ Participants can stop at any time without penalty
**Anonymity**: ✅ Only session IDs recorded, no names/emails in data files
**Data retention**: ✅ Data stored locally, deleted after analysis (6 weeks max)
**PII in screenshots**: ✅ All screenshots cropped/scrubbed before submission

---

## 8. Timeline

| Activity | Date/Time | Duration |
|----------|-----------|----------|
| Finalize protocol & tasks | Week 9 Lab 1 | 1 hour |
| Recruit participants | Week 9 (before Lab 2) | 2 days |
| Conduct pilots | Week 9 Lab 2 | 2 hours |
| Debrief & synthesise findings | Week 9 Lab 2 | 30 mins |
| Assemble Task 1 pack | Week 9 Lab 2 (end) | 20 mins |
| Submit to Gradescope | Week 9 Lab 2 deadline | - |

---

## 9. Expected Outcomes

**What will you learn from this evaluation?**
- [Design question 1 answered]
- [Design question 2 answered]
- [Accessibility barriers identified]
- [Performance bottlenecks quantified]

**How will results inform redesign?**
- Findings will be prioritised using (Impact + Inclusion) – Effort matrix
- Top 3 issues will be addressed in Week 10 redesign (Task 2)
- Evidence chains will link raw data → findings → backlog items → fixes

---

## References

- [Nielsen: Usability Testing 101](https://www.nngroup.com/articles/usability-testing-101/)
- [W3C: Measuring Accessibility](https://www.w3.org/WAI/test-evaluate/metrics/)
- Week 9 Lab 1 materials: `wk09/lab-wk9/research/`
